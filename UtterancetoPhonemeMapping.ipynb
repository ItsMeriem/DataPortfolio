{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR4qfYrVoO4v"
      },
      "source": [
        "<center>\n",
        "    <h1>Utterance to Phoneme Mapping</h1>\n",
        "    <h2>A Speech Recognition Task</h2>\n",
        "    <h3>Meriem Fouad</h3>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction\n",
        "\n",
        "This project is in the field of speech recognition. The goal is to predict the phonemes of a recording. The picture below illustrates the task at hand, where an audio recording of a person saying the word “yes” has been passed though a neural network, which outputs the phonetic transcription, /Y/ /EH/ /S/. My task is to build a recurrent neural network that performs this task.\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"yes_example.png\" width=\"250\">\n",
        "</p>\n",
        "\n",
        "## Data and Alignment Challenge:\n",
        "\n",
        "The data consists of speech recordings that are parametrized as a sequence of feature vectors (mel spectral vectors, each representing a “frame” of speech), which arrive at a rate of 100 frames per second. \n",
        "\n",
        "The challenge here is that the output and input are not time-synchronous. It is unknown a priori which phonemes occur in the output, the length of the output sequence, or even when to output the phonemes the recording. The image below illustrates this issue: the input to the model is a sequence of feature vectors (shown by the red boxes) from a speech recording. The network must analyze the input sequence and generate the output phoneme sequence (e.g. “/Y/ /EH/ /S/”) that best matches the audio. The output is not time-synchronous with the input. \n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"alignment_issue.png\" width=\"250\">\n",
        "</p>\n",
        "\n",
        "## Connectionist Temporal Classification\n",
        "\n",
        "To tackle this alignment we issue, we use a **Connectionist Temporal Classification** (CTC) where we decompose the inference into a 2-step process:\n",
        "\n",
        "1. **Step 1** A neural network generates outputs at every time step. This eliminates the uncertainty of knowing when to generate the (sporadic) outputs\n",
        "2. **Step 2** We perform a **dynamic programming (DP)** search-like operation on the complete set of outputs generated by the network, to generate the actual final outputs\n",
        "\n",
        "Here's an illustration of the CTC framework:\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"ctc.png\" width=\"250\">\n",
        "</p>\n",
        "\n",
        "\n",
        "## Network :\n",
        "\n",
        "My Automatic Speech Recognition network consists of the following architecture:\n",
        "\n",
        "1. Encoder:\n",
        "  a. Embedding with 4 CNNs and Relu activation,\n",
        "  b. 1 Bi-directional Long Short-Term Memory (1 Bi-LSTM)\n",
        "  c. 2 pBlstms (pyramidal Bi-LSTMs)\n",
        "\n",
        "2. Decoder: MLP \n",
        "\n",
        "3. Inference: Beam Search\n",
        "\n",
        "## Hyper Parameter Experimentation\n",
        "\n",
        "1. **Data Augmentation**: Experimented with Frequency and Time masking\n",
        "\n",
        "2. **Additional Params**: \n",
        "\n",
        "    I experimented with a variety of parameters including: beam width, batch size, adam weight decay rate, locked dropout, embed size, scheduler type and initialization linear rate.\n",
        "\n",
        "    I logged my experiments on wandb. I'm adding below how a variety of my models performed.\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"wandb.png\" width=\"700\">\n",
        "</p>\n",
        "\n",
        "To see the wandb log of experiments, go to [this link](https://wandb.ai/mfouad-cmu/hw3p2/table)\n",
        "\n",
        "\n",
        "## Evaluation Metric:\n",
        "\n",
        "We used the Levenshtein Distance as the main evaluation metrics. Levenshtein Distance takes as input two strings, and the computed distance is the number of character differences between the strings.\n",
        "\n",
        "\n",
        "\n",
        "#### Note: \n",
        "This project was part of a Deep Learning course (11-785: Introduction to Deep Learning) I took at Carnegie Mellon University during Fall 2024.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVrl4EPC6cUD"
      },
      "source": [
        "# Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mA9qZoIDcx-h",
        "outputId": "0840b076-af65-4cb8-c399-10bdb094fb1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m889.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.3/24.3 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchtext==0.14.1 torchaudio==0.13.1 torchdata==0.5.1 --extra-index-url https://download.pytorch.org/whl/cu117 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SS7a7xeEoaV9",
        "outputId": "ee2c04ba-1786-4595-e971-404aaae33af4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchsummaryX==1.3.0\n",
            "  Downloading torchsummaryX-1.3.0-py3-none-any.whl.metadata (325 bytes)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchsummaryX==1.3.0) (1.13.1+cu117)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchsummaryX==1.3.0) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from torchsummaryX==1.3.0) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->torchsummaryX==1.3.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->torchsummaryX==1.3.0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->torchsummaryX==1.3.0) (2024.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchsummaryX==1.3.0) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->torchsummaryX==1.3.0) (1.16.0)\n",
            "Downloading torchsummaryX-1.3.0-py3-none-any.whl (3.6 kB)\n",
            "Installing collected packages: torchsummaryX\n",
            "Successfully installed torchsummaryX-1.3.0\n",
            "Collecting pandas==1.5.2\n",
            "  Downloading pandas-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.2) (2024.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.2) (1.26.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.2) (1.16.0)\n",
            "Downloading pandas-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 1.24.0 requires pandas>=1.5.3, but you have pandas 1.5.2 which is incompatible.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 1.5.2 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.2 which is incompatible.\n",
            "ibis-framework 9.2.0 requires pandas<3,>=1.5.3, but you have pandas 1.5.2 which is incompatible.\n",
            "mizani 0.11.4 requires pandas>=2.1.0, but you have pandas 1.5.2 which is incompatible.\n",
            "plotnine 0.13.6 requires pandas<3.0.0,>=2.1.0, but you have pandas 1.5.2 which is incompatible.\n",
            "xarray 2024.9.0 requires pandas>=2.1, but you have pandas 1.5.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-1.5.2\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install torchsummaryX==1.3.0\n",
        "!pip install pandas==1.5.2\n",
        "!pip install wandb --quiet\n",
        "!pip install python-Levenshtein -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4r6vc6t6cUF",
        "outputId": "ea919031-f786-4745-dc8a-5712f0f6aea1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'ctcdecode'...\n",
            "remote: Enumerating objects: 1102, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 1102 (delta 16), reused 32 (delta 14), pack-reused 1063 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1102/1102), 782.27 KiB | 2.20 MiB/s, done.\n",
            "Resolving deltas: 100% (529/529), done.\n",
            "Submodule 'third_party/ThreadPool' (https://github.com/progschj/ThreadPool.git) registered for path 'third_party/ThreadPool'\n",
            "Submodule 'third_party/kenlm' (https://github.com/kpu/kenlm.git) registered for path 'third_party/kenlm'\n",
            "Cloning into '/content/ctcdecode/third_party/ThreadPool'...\n",
            "remote: Enumerating objects: 82, done.        \n",
            "remote: Counting objects: 100% (26/26), done.        \n",
            "remote: Compressing objects: 100% (9/9), done.        \n",
            "Receiving objects: 100% (82/82), 13.34 KiB | 297.00 KiB/s, done.\n",
            "Resolving deltas: 100% (36/36), done.\n",
            "remote: Total 82 (delta 19), reused 17 (delta 17), pack-reused 56 (from 1)        \n",
            "Cloning into '/content/ctcdecode/third_party/kenlm'...\n",
            "remote: Enumerating objects: 14170, done.        \n",
            "remote: Counting objects: 100% (483/483), done.        \n",
            "remote: Compressing objects: 100% (337/337), done.        \n",
            "remote: Total 14170 (delta 167), reused 410 (delta 132), pack-reused 13687 (from 1)        \n",
            "Receiving objects: 100% (14170/14170), 5.91 MiB | 8.53 MiB/s, done.\n",
            "Resolving deltas: 100% (8047/8047), done.\n",
            "Submodule path 'third_party/ThreadPool': checked out '9a42ec1329f259a5f4881a291db1dcb8f2ad9040'\n",
            "Submodule path 'third_party/kenlm': checked out '35835f1ac4884126458ac89f9bf6dd9ccad561e0'\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!git clone --recursive https://github.com/parlance/ctcdecode.git\n",
        "!pip install wget -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFALf3R76cUF",
        "outputId": "9931b427-f2f7-480f-e794-febd23cdbbe6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/ctcdecode\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ctcdecode (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd ctcdecode\n",
        "!pip install . -q\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWVONJxCobPc"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78ZTCIXoof2f",
        "outputId": "4ccb7d8f-6eae-47bd-e472-455ad546efd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummaryX import summary\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import torchaudio.transforms as tat\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "# imports for decoding and distance calculation\n",
        "import ctcdecode\n",
        "import Levenshtein\n",
        "from ctcdecode import CTCBeamDecoder\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg3-yJ8tok34"
      },
      "source": [
        "# Kaggle Competition and Data Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdUelfGhom1m",
        "outputId": "195ca459-eaf9-4fbb-cfca-682ae9951776"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/59.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8 -q\n",
        "!mkdir /root/.kaggle\n",
        "\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "    f.write('{\"username\":\".....\",\"key\":\"......\"}')\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSjBwfXeoq4B",
        "outputId": "319a2812-59a1-44b3-d247-873149bced21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading 11-785-hw3p2-f24.zip to /content\n",
            "100% 3.96G/3.97G [00:49<00:00, 67.3MB/s]\n",
            "100% 3.97G/3.97G [00:49<00:00, 85.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle competitions download -c 11-785-hw3p2-f24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ruxWP60LCQA",
        "outputId": "f569792a-9417-48d0-f3b6-ee0bf9492320"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11785-f24-hw3p2  11-785-hw3p2-f24.zip  ctcdecode  sample_data\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "This will take a couple minutes, but you should see at least the following:\n",
        "11-785-f24-hw3p2  ctcdecode  hw3p2asr-f24.zip  sample_data\n",
        "'''\n",
        "!unzip -q 11-785-hw3p2-f24.zip\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ORNHnSFroP0"
      },
      "source": [
        "# Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "k0v7wHRWrqH6"
      },
      "outputs": [],
      "source": [
        "# ARPABET PHONEME MAPPING\n",
        "\n",
        "CMUdict_ARPAbet = {\n",
        "    \"\" : \" \",\n",
        "    \"[SIL]\": \"-\", \"NG\": \"G\", \"F\" : \"f\", \"M\" : \"m\", \"AE\": \"@\",\n",
        "    \"R\"    : \"r\", \"UW\": \"u\", \"N\" : \"n\", \"IY\": \"i\", \"AW\": \"W\",\n",
        "    \"V\"    : \"v\", \"UH\": \"U\", \"OW\": \"o\", \"AA\": \"a\", \"ER\": \"R\",\n",
        "    \"HH\"   : \"h\", \"Z\" : \"z\", \"K\" : \"k\", \"CH\": \"C\", \"W\" : \"w\",\n",
        "    \"EY\"   : \"e\", \"ZH\": \"Z\", \"T\" : \"t\", \"EH\": \"E\", \"Y\" : \"y\",\n",
        "    \"AH\"   : \"A\", \"B\" : \"b\", \"P\" : \"p\", \"TH\": \"T\", \"DH\": \"D\",\n",
        "    \"AO\"   : \"c\", \"G\" : \"g\", \"L\" : \"l\", \"JH\": \"j\", \"OY\": \"O\",\n",
        "    \"SH\"   : \"S\", \"D\" : \"d\", \"AY\": \"Y\", \"S\" : \"s\", \"IH\": \"I\",\n",
        "    \"[SOS]\": \"[SOS]\", \"[EOS]\": \"[EOS]\"\n",
        "}\n",
        "\n",
        "\n",
        "CMUdict = list(CMUdict_ARPAbet.keys())\n",
        "ARPAbet = list(CMUdict_ARPAbet.values())\n",
        "\n",
        "\n",
        "PHONEMES = CMUdict[:-2]\n",
        "LABELS = ARPAbet[:-2]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agmNBKf4JrLV"
      },
      "source": [
        "### Train Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "afd0_vlbJmr_"
      },
      "outputs": [],
      "source": [
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "\n",
        "\n",
        "    def __init__(self, root, transformations, phonemes = PHONEMES, partition = \"train-clean-100\",\n",
        "                 val_data = False):\n",
        "\n",
        "        # Load the directory and all files in them\n",
        "\n",
        "        self.mfcc_dir = f\"{root}/{partition}/mfcc\"\n",
        "        self.transcript_dir = f\"{root}/{partition}/transcript\"\n",
        "\n",
        "        # Important: Load the files in sorted order of their names in the directory.\n",
        "        mfcc_names = sorted(os.listdir(self.mfcc_dir))\n",
        "        transcript_names = sorted(os.listdir(self.transcript_dir))\n",
        "\n",
        "        self.phonemes = phonemes\n",
        "        self.val_data = val_data\n",
        "        self.transformations = transformations\n",
        "\n",
        "        # Making sure that we have the same no. of mfcc and transcripts\n",
        "        assert len(mfcc_names) == len(transcript_names)\n",
        "\n",
        "\n",
        "        self.mfccs, self.transcripts = [], []\n",
        "\n",
        "        for i in range(len(mfcc_names)):\n",
        "        # Load a single mfcc\n",
        "            mfcc        = np.load(f\"{self.mfcc_dir}/{mfcc_names[i]}\")\n",
        "            #  Do Cepstral Normalization of mfcc \n",
        "            mfcc = (mfcc - np.mean(mfcc, axis=0)) / np.std(mfcc, axis=0)\n",
        "            # Remove [SOS] and [EOS] from the transcript\n",
        "            transcript  = np.load(f\"{self.transcript_dir}/{transcript_names[i]}\")[1:-1]\n",
        "            #  Append each mfcc to self.mfcc, transcript to self.transcript\n",
        "            self.mfccs.append(mfcc)\n",
        "\n",
        "            # Map these phonemes to integers\n",
        "            # Create a dictionary from the list where they keys are the phenome and the values are the indices,\n",
        "            transcript = np.vectorize(dict(zip(self.phonemes, range(len(self.phonemes)))).__getitem__)(transcript)\n",
        "\n",
        "            self.transcripts.append(transcript)\n",
        "\n",
        "        # Length of the dataset is now the length of concatenated mfccs/transcripts\n",
        "        self.length = len(self.mfccs)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        '''\n",
        "        RETURNs THE MFCC COEFFICIENTS AND ITS CORRESPONDING LABELS\n",
        "        '''\n",
        "\n",
        "        mfcc = torch.FloatTensor(self.mfccs[ind])\n",
        "        transcript = torch.tensor(self.transcripts[ind])\n",
        "        return mfcc, transcript\n",
        "\n",
        "\n",
        "    def collate_fn(self,batch):\n",
        "        \"\"\"\n",
        "        \n",
        "        Collate function for batching sequences.\n",
        "        \n",
        "        This function extracts features and labels from the input batch, pads them to the same length, \n",
        "        and applies transformations for efficiency.\n",
        "\n",
        "\n",
        "        Args:\n",
        "            batch (list): A list of tuples containing (features, labels).\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing:\n",
        "                - Padded features (Tensor)\n",
        "                - Padded labels (Tensor)\n",
        "                - Lengths of features (Tensor)\n",
        "                - Lengths of labels (Tensor)\n",
        "        \"\"\"\n",
        "\n",
        "        # batch of input mfcc coefficients\n",
        "        batch_mfcc = [item[0] for item in batch]\n",
        "        # batch of output phonemes\n",
        "        batch_transcript = [item[1] for item in batch]\n",
        "\n",
        "\n",
        "        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first = True)\n",
        "        lengths_mfcc = [item.shape[0] for item in batch_mfcc]\n",
        "\n",
        "        batch_transcript_pad = pad_sequence(batch_transcript, batch_first = True)\n",
        "        lengths_transcript = [item.shape[0] for item in batch_transcript]\n",
        "\n",
        "\n",
        "        # Only apply transformations to train data (Not val):\n",
        "        if self.val_data == False:\n",
        "            for transform in self.transformations:\n",
        "                batch_mfcc_pad = transform(batch_mfcc_pad)\n",
        "\n",
        "        return batch_mfcc_pad, batch_transcript_pad, torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqDrxeHfJw4g"
      },
      "source": [
        "### Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HrLS1wfVJppA"
      },
      "outputs": [],
      "source": [
        "# Test Dataloader\n",
        "\n",
        "class AudioDatasetTest(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, root, phonemes = PHONEMES, partition= \"test-clean\"):\n",
        "\n",
        "        self.phonemes   = phonemes\n",
        "\n",
        "        self.mfcc_dir       = f\"{root}/{partition}/mfcc\"\n",
        "\n",
        "        # List files in sefl.mfcc_dir using os.listdir in sorted order\n",
        "        mfcc_names          = sorted(os.listdir(self.mfcc_dir))\n",
        "\n",
        "        self.mfccs = []\n",
        "\n",
        "        for i in range(len(mfcc_names)):\n",
        "            mfcc        = np.load(f\"{self.mfcc_dir}/{mfcc_names[i]}\")\n",
        "        #   Do Cepstral Normalization of mfcc\n",
        "            mfcc = (mfcc - np.mean(mfcc, axis=0)) / np.std(mfcc, axis=0)\n",
        "\n",
        "            self.mfccs.append(mfcc)\n",
        "\n",
        "        # Length of the dataset is now the length of concatenated mfccs\n",
        "        self.length = len(self.mfccs)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "\n",
        "        mfcc = torch.FloatTensor(self.mfccs[ind])\n",
        "\n",
        "        return mfcc\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "\n",
        "        batch_mfcc_pad = pad_sequence(batch, batch_first = True)\n",
        "        lengths_mfcc = [item.shape[0] for item in batch]\n",
        "\n",
        "        return batch_mfcc_pad, torch.tensor(lengths_mfcc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt-veYcdL6Fe"
      },
      "source": [
        "### Config - Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MN82c3KpLup8"
      },
      "outputs": [],
      "source": [
        "root = '/content/11785-f24-hw3p2'\n",
        "# root = '/kaggle/working/11785-f24-hw3p2'\n",
        "\n",
        "# Update this based on desired experimentations\n",
        "config = {\n",
        "    \"beam_width\" : 10,\n",
        "    \"init_lr\"         : 1e-3,\n",
        "    \"epochs\"     : 25,\n",
        "    \"batch_size\" : 32, # Increase if device can handle it\n",
        "    'adamw_decay': 7.5e-3,\n",
        "}\n",
        "\n",
        "\n",
        "freq_masking = tat.FrequencyMasking(freq_mask_param=5, iid_masks = True)\n",
        "time_masking = tat.TimeMasking(time_mask_param=5, iid_masks = True)\n",
        "\n",
        "transforms = [freq_masking, time_masking] # set of tranformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmuPk9J6L8dz"
      },
      "source": [
        "### Data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_kG0gU2x4hH",
        "outputId": "40e62e1d-befc-496d-e467-ba85c3ef9a3f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# get me RAMMM!!!!\n",
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mzoYfTKu14s",
        "outputId": "90f571d9-3e52-4de8-9f37-e866f195f774"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch size:  32\n",
            "Train dataset samples = 28539, batches = 892\n",
            "Val dataset samples = 2703, batches = 85\n",
            "Test dataset samples = 2620, batches = 82\n"
          ]
        }
      ],
      "source": [
        "# Create objects for the dataset class\n",
        "train_data = AudioDataset(root = root, transformations = transforms)\n",
        "val_data = AudioDataset(root = root, transformations = transforms, partition= \"dev-clean\", val_data=True)\n",
        "test_data = AudioDatasetTest(root = root, partition=\"test-clean\")\n",
        "\n",
        "# Do NOT forget to pass in the collate function as parameter while creating the dataloader\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = train_data,\n",
        "    num_workers = 4,\n",
        "    batch_size  = config['batch_size'],\n",
        "    pin_memory  = True,\n",
        "    shuffle     = True,\n",
        "    collate_fn = train_data.collate_fn\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = val_data,\n",
        "    num_workers = 2,\n",
        "    batch_size  = config['batch_size'],\n",
        "    pin_memory  = True,\n",
        "    shuffle     = False,\n",
        "    collate_fn = val_data.collate_fn\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = test_data,\n",
        "    num_workers = 2,\n",
        "    batch_size  = config['batch_size'],\n",
        "    pin_memory  = True,\n",
        "    shuffle     = False,\n",
        "    collate_fn = test_data.collate_fn\n",
        ")\n",
        "\n",
        "print(\"Batch size: \", config['batch_size'])\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXMtwyviKaxK",
        "outputId": "bcf764eb-258b-4153-a92f-c8ef26c4d804"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 1642, 28]) torch.Size([32, 219]) torch.Size([32]) torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "# sanity check\n",
        "for data in train_loader:\n",
        "    x, y, lx, ly = data\n",
        "    print(x.shape, y.shape, lx.shape, ly.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PENzKgAuGx40",
        "outputId": "b7752d35-5188-4184-b8e7-73fd0fe12666"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 1675, 28]) torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "# sanity check\n",
        "for data in test_loader:\n",
        "    x, lx = data\n",
        "    print(x.shape, lx.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSexxhdfMUzx"
      },
      "source": [
        "# NETWORK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DK1xFTY9wohG"
      },
      "outputs": [],
      "source": [
        "# Locked DROPOUT (increases performance!)\n",
        "\n",
        "class LockedDropout(nn.Module):\n",
        "    def __init__(self, dropout = 0.5):\n",
        "        super(LockedDropout, self).__init__()\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.training or self.dropout == 0:\n",
        "            return x\n",
        "        x, x_lens = pad_packed_sequence(x, batch_first=True)\n",
        "        m = x.new_empty(1, x.size(1), x.size(2), requires_grad=False).bernoulli_(1 - self.dropout)\n",
        "        mask = m / (1 - self.dropout)\n",
        "        mask = mask.expand_as(x)\n",
        "        x = mask * x\n",
        "\n",
        "        x = pack_padded_sequence(x, x_lens, batch_first=True, enforce_sorted=False)\n",
        "        return x\n",
        "\n",
        "# Reference: \n",
        "# Locked Dropout Code Inspired From:\n",
        "# https://github.com/salesforce/awd-lstm-lm/blob/dfd3cb0235d2caf2847a4d53e1cbd495b781b5d2/locked_dropout.py#L5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-qb7wnAzCZl"
      },
      "source": [
        "## ASR Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB6eh3gnMUzy"
      },
      "source": [
        "### Pyramid Bi-LSTM (pBLSTM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qd4BEX_yMUzz"
      },
      "outputs": [],
      "source": [
        "# Utils for network\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "class PermuteBlock(torch.nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.transpose(1, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OmdyXI6KMUzz"
      },
      "outputs": [],
      "source": [
        "from re import template\n",
        "class pBLSTM(torch.nn.Module):\n",
        "\n",
        "    '''\n",
        "    Pyramidal BiLSTM\n",
        "\n",
        "    At each step,\n",
        "    1. Pad your input if it is packed (Unpack it)\n",
        "    2. Reduce the input length dimension by concatenating feature dimension\n",
        "    3. Pack input\n",
        "    4. Pass it into LSTM layer\n",
        "\n",
        "    To make our implementation modular, we pass 1 layer at a time.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(pBLSTM, self).__init__()\n",
        "\n",
        "        # Initialize a single layer bidirectional LSTM with the given input_size and hidden_size\n",
        "        self.blstm = nn.LSTM(input_size = 2*input_size, hidden_size = hidden_size, num_layers = 1,\n",
        "                             batch_first = True, bidirectional = True)\n",
        "\n",
        "\n",
        "    def forward(self, x_packed): # x_packed is a PackedSequence\n",
        "\n",
        "        # Pad Packed Sequence\n",
        "        x_unpacked, x_lens = pad_packed_sequence(x_packed, batch_first=True)\n",
        "\n",
        "        # Call self.trunc_reshape() which downsamples the time steps of x and increases the feature dimensions\n",
        "        # self.trunc_reshape will return 2 outputs\n",
        "        x, x_lens = self.trunc_reshape(x_unpacked, x_lens)\n",
        "\n",
        "        # Pack Padded Sequence\n",
        "        x_packed = pack_padded_sequence(x, x_lens, batch_first = True, enforce_sorted=False)\n",
        "\n",
        "        # Pass the sequence through bLSTM\n",
        "        out, _ = self.blstm(x_packed)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def trunc_reshape(self, x, x_lens):\n",
        "        # Handle cases with odd number of timesteps\n",
        "        if x.shape[1] % 2 != 0:\n",
        "          x = x[:, :-1, :] # trim to an even length by deleting the final vector\n",
        "\n",
        "\n",
        "        # Reshape x. When reshaping x, you have to reduce number of timesteps by a downsampling factor\n",
        "        # while increasing number of features by the same factor\n",
        "        x_downsampled = torch.reshape(input = x, shape = (x.shape[0], x.shape[1]//2, x.shape[2]*2))\n",
        "\n",
        "        # Reduce lengths by the same downsampling factor\n",
        "        x_lens = x_lens // 2\n",
        "        return x_downsampled, x_lens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3ZQ75OcMUz0"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GEzw5_xmMUz0"
      },
      "outputs": [],
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "    '''\n",
        "    The Encoder takes utterances as inputs and returns latent feature representations\n",
        "    '''\n",
        "    def __init__(self, input_size, encoder_hidden_size):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.embedding = torch.nn.Sequential(\n",
        "\n",
        "            PermuteBlock(),\n",
        "\n",
        "            nn.Conv1d(in_channels = input_size, out_channels = 64, kernel_size = 5, padding = 2, stride = 1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.BatchNorm1d(64),\n",
        "\n",
        "            nn.Conv1d(in_channels = 64, out_channels = 128, kernel_size = 5, padding = 2, stride = 1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.BatchNorm1d(128),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Conv1d(in_channels = 128, out_channels = 256, kernel_size = 5, padding = 2, stride = 1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.BatchNorm1d(256),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            torch.nn.Conv1d(in_channels = 256, out_channels = input_size, kernel_size = 5,\n",
        "                            stride = 1, padding = 2),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.BatchNorm1d(input_size),\n",
        "            nn.Dropout(0.1),\n",
        "\n",
        "            PermuteBlock()\n",
        "\n",
        "        )\n",
        "\n",
        "        self.BLSTM = nn.LSTM(input_size = input_size, hidden_size = encoder_hidden_size,\n",
        "                             num_layers = 4, batch_first=True, bidirectional=True)\n",
        "\n",
        "\n",
        "        self.pBLSTMs = torch.nn.Sequential( #\n",
        "            pBLSTM(input_size = 2*encoder_hidden_size, hidden_size = encoder_hidden_size),\n",
        "            LockedDropout(dropout=0.3),\n",
        "            pBLSTM(input_size = encoder_hidden_size*2, hidden_size = encoder_hidden_size),\n",
        "            LockedDropout(dropout=0.3)\n",
        "\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, x, x_lens):\n",
        "        # Call the embedding layer\n",
        "        x = self.embedding(x)\n",
        "        # Pack Padded Sequence\n",
        "        x_packed = pack_padded_sequence(x, x_lens, batch_first = True, enforce_sorted = False)\n",
        "        # Pass through BLSTM layer\n",
        "        out, _ = self.BLSTM(x_packed)\n",
        "        # Pass Sequence through the pyramidal Bi-LSTM layer\n",
        "        out = self.pBLSTMs(out)\n",
        "        # Pad Packed Sequence\n",
        "        encoder_outputs, encoder_lens = pad_packed_sequence(out, batch_first = True)\n",
        "\n",
        "        return encoder_outputs, encoder_lens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg82HXa3MUz1"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PQIRxdNTMUz1"
      },
      "outputs": [],
      "source": [
        "class Decoder(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, embed_size, output_size= 41):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mlp = torch.nn.Sequential(\n",
        "            nn.Linear(2*embed_size, 2*embed_size),\n",
        "            nn.ReLU(),\n",
        "            PermuteBlock(),\n",
        "            torch.nn.BatchNorm1d(2*embed_size),\n",
        "            PermuteBlock(),\n",
        "            nn.Linear(2*embed_size, output_size),\n",
        "        )\n",
        "\n",
        "        self.softmax = torch.nn.LogSoftmax(dim=2)\n",
        "\n",
        "    def forward(self, encoder_out):\n",
        "        # call MLP\n",
        "        out = self.mlp(encoder_out)\n",
        "        out = self.softmax(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qmHf6pFiMUz1"
      },
      "outputs": [],
      "source": [
        "class ASRModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, embed_size= 192, output_size= len(PHONEMES)):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder        = Encoder(input_size=input_size, encoder_hidden_size = embed_size)\n",
        "        self.decoder        = Decoder(embed_size = embed_size, output_size = output_size)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, lengths_x):\n",
        "\n",
        "        encoder_out, encoder_lens   = self.encoder(x, lengths_x)\n",
        "        decoder_out                 = self.decoder(encoder_out)\n",
        "\n",
        "        return decoder_out, encoder_lens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EV7DMPDoMUz2"
      },
      "source": [
        "## Initialize ASR Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaaDsnnLMUz2",
        "outputId": "94cfcb41-615b-46a8-b48f-6efd1c7866a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ASRModel(\n",
            "  (encoder): Encoder(\n",
            "    (embedding): Sequential(\n",
            "      (0): PermuteBlock()\n",
            "      (1): Conv1d(28, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "      (2): ReLU()\n",
            "      (3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (4): Conv1d(64, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "      (5): ReLU()\n",
            "      (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (7): Dropout(p=0.3, inplace=False)\n",
            "      (8): Conv1d(128, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "      (9): ReLU()\n",
            "      (10): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (11): Dropout(p=0.2, inplace=False)\n",
            "      (12): Conv1d(256, 28, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "      (13): ReLU()\n",
            "      (14): BatchNorm1d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (15): Dropout(p=0.1, inplace=False)\n",
            "      (16): PermuteBlock()\n",
            "    )\n",
            "    (BLSTM): LSTM(28, 256, num_layers=4, batch_first=True, bidirectional=True)\n",
            "    (pBLSTMs): Sequential(\n",
            "      (0): pBLSTM(\n",
            "        (blstm): LSTM(1024, 256, batch_first=True, bidirectional=True)\n",
            "      )\n",
            "      (1): LockedDropout()\n",
            "      (2): pBLSTM(\n",
            "        (blstm): LSTM(1024, 256, batch_first=True, bidirectional=True)\n",
            "      )\n",
            "      (3): LockedDropout()\n",
            "    )\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (mlp): Sequential(\n",
            "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): PermuteBlock()\n",
            "      (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (4): PermuteBlock()\n",
            "      (5): Linear(in_features=512, out_features=41, bias=True)\n",
            "    )\n",
            "    (softmax): LogSoftmax(dim=2)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = ASRModel(\n",
        "    input_size  = 28,\n",
        "    embed_size  = 256,\n",
        "    output_size = len(PHONEMES)\n",
        ").to(device)\n",
        "print(model)\n",
        "#summary(model, x.to(device), lx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBwunYpyugFg"
      },
      "source": [
        "# Training Config\n",
        "Initialize Loss Criterion, Optimizer, CTC Beam Decoder, Scheduler, Scaler (Mixed-Precision), etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "iGoozH2nd6KB"
      },
      "outputs": [],
      "source": [
        "# Define CTC loss as the criterion\n",
        "# More on CTC Loss: https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html\n",
        "criterion = nn.CTCLoss(blank = 0, reduction = 'mean') ## We want expected loss so I'm using the mean\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=config['init_lr'],\n",
        "                             weight_decay = config['adamw_decay'])\n",
        "\n",
        "# Declare the decoder. Use the CTC Beam Decoder to decode phonemes\n",
        "# More on CTC Beam Decoder Doc: https://github.com/parlance/ctcdecode\n",
        "decoder =  CTCBeamDecoder(LABELS, beam_width=config['beam_width'], log_probs_input = True) # reminder to use a bigger beam width for testing\n",
        "\n",
        "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.9)\n",
        "# Next try a cosine annealing w/ 1e-5 lr\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=5, verbose=True)\n",
        "\n",
        "# Mixed Precision, if you need it\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jmc6_4eWL2Xp"
      },
      "source": [
        "# Decode Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "KHjnCDddL36E"
      },
      "outputs": [],
      "source": [
        "def decode_prediction(output, output_lens, decoder, PHONEME_MAP= LABELS):\n",
        "\n",
        "    beam_results, beam_scores, timesteps, out_lens = decoder.decode(output,\n",
        "                                                                    seq_lens= output_lens)\n",
        "\n",
        "    pred_strings                    = []\n",
        "\n",
        "    # print(\"this is the len of pred strings\", output_lens.shape[0])\n",
        "    for i in range(output_lens.shape[0]):\n",
        "\n",
        "        #Create the prediction from the output of decoder.decode. Map it using PHONEME_MAP.\n",
        "        top_beam = beam_results[i][0][:out_lens[i][0]]\n",
        "\n",
        "        top_beam = \"\".join([PHONEME_MAP[i] for i in top_beam])\n",
        "\n",
        "        pred_strings.append(top_beam)\n",
        "\n",
        "    return pred_strings\n",
        "\n",
        "def calculate_levenshtein(output, label, output_lens, label_lens, decoder,\n",
        "                          PHONEME_MAP= LABELS): # y - sequence of integers\n",
        "\n",
        "    dist            = 0\n",
        "    batch_size      = label.shape[0]\n",
        "\n",
        "    pred_strings    = decode_prediction(output, output_lens, decoder, PHONEME_MAP)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        # Get predicted string and label string for each element in the batch\n",
        "        pred_string = pred_strings[i][:label_lens[i]]\n",
        "        label_string = \"\".join(PHONEME_MAP[j] for j in label[i])[:label_lens[i]]\n",
        "\n",
        "        dist += Levenshtein.distance(pred_string, label_string)\n",
        "\n",
        "    dist /= batch_size\n",
        "    return dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Qk9iZud1LXT"
      },
      "source": [
        "# Test Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnTLL-5gMBrY",
        "outputId": "a75f3121-358c-4788-f47b-de805ff24b47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 734, 41])\n",
            "torch.Size([734, 32, 41]) torch.Size([32, 265])\n",
            "tensor(7.5534, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "69.25\n"
          ]
        }
      ],
      "source": [
        "# test code to check shapes\n",
        "\n",
        "model.eval()\n",
        "for i, data in enumerate(val_loader, 0):\n",
        "    x, y, lx, ly = data\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    h, lh = model(x, lx)\n",
        "    print(h.shape)\n",
        "    h = torch.permute(h, (1, 0, 2))\n",
        "    print(h.shape, y.shape)\n",
        "    loss = criterion(h, y, lh, ly)\n",
        "    print(loss)\n",
        "\n",
        "    print(calculate_levenshtein(torch.permute(h, (1, 0, 2)), y, lx, ly, decoder, LABELS))\n",
        "\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd5aNaLVoR_g"
      },
      "source": [
        "# WandB\n",
        "\n",
        "You will need to fetch your api key from wandb.ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiDduMaDIARE",
        "outputId": "d0048771-3191-40ab-9fe8-46ab55f7ead6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmfouad\u001b[0m (\u001b[33mmfouad-cmu\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "# enter your wandb key here\n",
        "wandb.login(key=\".....\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "4s52yBOvICPZ",
        "outputId": "18031b9b-2824-4923-fdda-ca324cffff94"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.18.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241103_180152-uq6xbtt2</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Resuming run <strong><a href='https://wandb.ai/mfouad-cmu/hw3p2/runs/uq6xbtt2' target=\"_blank\">highcutoff_model+highembed</a></strong> to <a href='https://wandb.ai/mfouad-cmu/hw3p2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mfouad-cmu/hw3p2' target=\"_blank\">https://wandb.ai/mfouad-cmu/hw3p2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mfouad-cmu/hw3p2/runs/uq6xbtt2' target=\"_blank\">https://wandb.ai/mfouad-cmu/hw3p2/runs/uq6xbtt2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_run_name = \"highcutoff_model+highembed\"\n",
        "run = wandb.init(\n",
        "    #name = model_run_name, ## Wandb creates random run names if you skip this field\n",
        "    #reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    id = 'uq6xbtt2',### Insert specific run id here if you want to resume a previous run\n",
        "    resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    project = \"hw3p2\", ### Projects should be created in your wandb account\n",
        "    config = config ### Wandb Config for your run\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fLLj5KIMMOe"
      },
      "source": [
        "# Train Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ri87MAdhMUz5"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(model, train_loader, criterion, optimizer):\n",
        "\n",
        "    model.train()\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True,\n",
        "                     leave=False, position=0, desc='Train')\n",
        "\n",
        "    total_loss = 0\n",
        "    for i, data in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        x, y, lx, ly = data\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            h, lh = model(x, lx)\n",
        "            h = torch.permute(h, (1, 0, 2))\n",
        "            loss = criterion(h, y, lh, ly)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "\n",
        "        batch_bar.update() # Update tqdm bar\n",
        "\n",
        "        # Another couple things needed for FP16.\n",
        "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
        "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
        "        scaler.update() # This is something added just for FP16\n",
        "\n",
        "        del x, y, lx, ly, h, lh, loss\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close() # You need this to close the tqdm bar\n",
        "\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "\n",
        "def validate_model(model, val_loader, decoder, phoneme_map= LABELS):\n",
        "\n",
        "    model.eval()\n",
        "    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "\n",
        "    total_loss = 0\n",
        "    vdist = 0\n",
        "\n",
        "    for i, data in enumerate(val_loader):\n",
        "\n",
        "        x, y, lx, ly = data\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            h, lh = model(x, lx)\n",
        "            h = torch.permute(h, (1, 0, 2))\n",
        "            loss = criterion(h, y, lh, ly)\n",
        "\n",
        "        total_loss += float(loss)\n",
        "        vdist += calculate_levenshtein(torch.permute(h, (1, 0, 2)), y, lh, ly, decoder, phoneme_map)\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(total_loss / (i + 1))), dist=\"{:.04f}\".format(float(vdist / (i + 1))))\n",
        "\n",
        "        batch_bar.update()\n",
        "\n",
        "        del x, y, lx, ly, h, lh, loss\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "    total_loss = total_loss/len(val_loader)\n",
        "    val_dist = vdist/len(val_loader)\n",
        "    return total_loss, val_dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpYExu4vT4_g"
      },
      "source": [
        "## Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "husa5_EYMUz6"
      },
      "outputs": [],
      "source": [
        "def save_model(model, optimizer, scheduler, metric, epoch, path):\n",
        "    torch.save(\n",
        "        {'model_state_dict'         : model.state_dict(),\n",
        "         'optimizer_state_dict'     : optimizer.state_dict(),\n",
        "         'scheduler_state_dict'     : scheduler.state_dict(),\n",
        "         metric[0]                  : metric[1],\n",
        "         'epoch'                    : epoch},\n",
        "         path\n",
        "    )\n",
        "\n",
        "def load_model(path, model, metric= 'valid_acc', optimizer= None, scheduler= None):\n",
        "\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    if optimizer != None:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    if scheduler != None:\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "    epoch   = checkpoint['epoch']\n",
        "    metric  = checkpoint[metric]\n",
        "\n",
        "    return [model, optimizer, scheduler, epoch, metric]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "tExvyl1BIdMC"
      },
      "outputs": [],
      "source": [
        "# This is for checkpointing, if doing it over multiple sessions\n",
        "\n",
        "last_epoch_completed = 25\n",
        "start = last_epoch_completed\n",
        "end = config[\"epochs\"]\n",
        "best_lev_dist = float(\"5.6141\") # if restarting from some checkpoint, use what you saw there.\n",
        "epoch_model_path = 'epoch_model.pth'# set the model path\n",
        "best_model_path = 'best_model.pth' # set best model path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFKTBs9pmUNu",
        "outputId": "5891c7bf-10d9-4bcc-baba-5ac226090417"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resuming from epoch 22\n"
          ]
        }
      ],
      "source": [
        "# Retrieve past model\n",
        "\n",
        "artifact = wandb.restore('best_model.pth', run_path='mfouad-cmu/hw3p2/uq6xbtt2').name\n",
        "checkpoint = torch.load(artifact, weights_only=False)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "try:\n",
        "    print(f\"Resuming from epoch {checkpoint['epoch']}\")\n",
        "except:\n",
        "    print(f\"Resuming\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "fa69b7611f6b49a88b6900b69fc8ba6b",
            "8864d67a1e7f4921a1e312d0a3bcc53c",
            "5c7417433cfa427c934e09f47d71b05f",
            "f3aa82186e194b60958d5c8450fbc4bf",
            "cd3475f964b94415a0f453b39f86009a",
            "d70a201ca3cc44069c00261de7d9757d",
            "080ed6e31b5e458e8640db753ced634f",
            "c52be6375d3d4b32b32af82aab3f0a3b"
          ]
        },
        "id": "JR43E28rM9Ak",
        "outputId": "3844d675-2c7d-4865-ceb2-303bcbac161f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 24/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.1755\t Learning Rate 0.0010000\n",
            "\tVal Dist 5.5226%\t Val Loss 0.2674\n",
            "Saved epoch model\n",
            "Saved best model\n",
            "\n",
            "Epoch: 25/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.1646\t Learning Rate 0.0010000\n",
            "\tVal Dist 5.3398%\t Val Loss 0.2630\n",
            "Saved epoch model\n",
            "Saved best model\n",
            "\n",
            "Epoch: 26/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.1591\t Learning Rate 0.0010000\n",
            "\tVal Dist 5.2346%\t Val Loss 0.2616\n",
            "Saved epoch model\n",
            "Saved best model\n",
            "\n",
            "Epoch: 27/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.1541\t Learning Rate 0.0010000\n",
            "\tVal Dist 5.3506%\t Val Loss 0.2667\n",
            "Saved epoch model\n",
            "\n",
            "Epoch: 28/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.1545\t Learning Rate 0.0010000\n",
            "\tVal Dist 5.4596%\t Val Loss 0.2715\n",
            "Saved epoch model\n",
            "\n",
            "Epoch: 29/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.1484\t Learning Rate 0.0010000\n",
            "\tVal Dist 5.4998%\t Val Loss 0.2801\n",
            "Saved epoch model\n",
            "\n",
            "Epoch: 30/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.1468\t Learning Rate 0.0010000\n",
            "\tVal Dist 5.4542%\t Val Loss 0.2725\n",
            "Saved epoch model\n",
            "\n",
            "Epoch: 31/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.1405\t Learning Rate 0.0010000\n",
            "\tVal Dist 5.3615%\t Val Loss 0.2766\n",
            "Saved epoch model\n",
            "\n",
            "Epoch: 32/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.1399\t Learning Rate 0.0010000\n",
            "\tVal Dist 5.2130%\t Val Loss 0.2695\n",
            "Saved epoch model\n",
            "Saved best model\n",
            "\n",
            "Epoch: 33/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.1332\t Learning Rate 0.0010000\n",
            "\tVal Dist 5.1273%\t Val Loss 0.2672\n",
            "Saved epoch model\n",
            "Saved best model\n",
            "\n",
            "Epoch: 34/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.1311\t Learning Rate 0.0010000\n",
            "\tVal Dist 5.2376%\t Val Loss 0.2719\n",
            "Saved epoch model\n",
            "\n",
            "Epoch: 35/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.1339\t Learning Rate 0.0010000\n",
            "\tVal Dist 5.1772%\t Val Loss 0.2667\n",
            "Saved epoch model\n",
            "\n",
            "Epoch: 36/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.1262\t Learning Rate 0.0010000\n",
            "\tVal Dist 5.1790%\t Val Loss 0.2646\n",
            "Saved epoch model\n",
            "\n",
            "Epoch: 37/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.1264\t Learning Rate 0.0010000\n",
            "\tVal Dist 5.1754%\t Val Loss 0.2775\n",
            "Saved epoch model\n",
            "\n",
            "Epoch: 38/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.1241\t Learning Rate 0.0010000\n",
            "\tVal Dist 5.1783%\t Val Loss 0.2683\n",
            "Saved epoch model\n",
            "\n",
            "Epoch: 39/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 00039: reducing learning rate of group 0 to 5.0000e-04.\n",
            "\tTrain Loss 0.1170\t Learning Rate 0.0010000\n",
            "\tVal Dist 5.2178%\t Val Loss 0.2702\n",
            "Saved epoch model\n",
            "\n",
            "Epoch: 40/40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.0931\t Learning Rate 0.0005000\n",
            "\tVal Dist 4.8452%\t Val Loss 0.2652\n",
            "Saved epoch model\n",
            "Saved best model\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa69b7611f6b49a88b6900b69fc8ba6b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='212.623 MB of 254.317 MB uploaded\\r'), FloatProgress(value=0.836054148737092, max=…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>████████████████▁</td></tr><tr><td>train_loss</td><td>█▇▇▆▆▆▆▅▅▄▄▄▄▄▄▃▁</td></tr><tr><td>valid_dist</td><td>█▆▅▆▇█▇▆▅▄▅▄▄▄▄▅▁</td></tr><tr><td>valid_loss</td><td>▃▂▁▃▅█▅▇▄▃▅▃▂▇▄▄▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>0.0005</td></tr><tr><td>train_loss</td><td>0.09311</td></tr><tr><td>valid_dist</td><td>4.84522</td></tr><tr><td>valid_loss</td><td>0.26523</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">highcutoff_model+highembed</strong> at: <a href='https://wandb.ai/mfouad-cmu/hw3p2/runs/uq6xbtt2' target=\"_blank\">https://wandb.ai/mfouad-cmu/hw3p2/runs/uq6xbtt2</a><br/> View project at: <a href='https://wandb.ai/mfouad-cmu/hw3p2' target=\"_blank\">https://wandb.ai/mfouad-cmu/hw3p2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20241103_180152-uq6xbtt2/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Training Loop\n",
        "config['epochs'] = 40\n",
        "\n",
        "for epoch in range(23, config['epochs']):\n",
        "\n",
        "    print(\"\\nEpoch: {}/{}\".format(epoch+1, config['epochs']))\n",
        "\n",
        "    curr_lr = float(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "    train_loss              = train_model(model, train_loader, criterion, optimizer)\n",
        "    valid_loss, valid_dist  = validate_model(model, val_loader, decoder, phoneme_map= LABELS)\n",
        "    scheduler.step(valid_dist)\n",
        "\n",
        "    print(\"\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_loss, curr_lr))\n",
        "    print(\"\\tVal Dist {:.04f}%\\t Val Loss {:.04f}\".format(valid_dist, valid_loss))\n",
        "\n",
        "    wandb.log({\n",
        "        'train_loss': train_loss,\n",
        "        'valid_dist': valid_dist,\n",
        "        'valid_loss': valid_loss,\n",
        "        'lr'        : curr_lr\n",
        "    })\n",
        "\n",
        "    save_model(model, optimizer, scheduler, ['valid_dist', valid_dist], epoch, epoch_model_path)\n",
        "    wandb.save(epoch_model_path)\n",
        "    print(\"Saved epoch model\")\n",
        "\n",
        "    if valid_dist <= best_lev_dist:\n",
        "        best_lev_dist = valid_dist\n",
        "        save_model(model, optimizer, scheduler, ['valid_dist', valid_dist], epoch, best_model_path)\n",
        "        wandb.save(best_model_path)\n",
        "        print(\"Saved best model\")\n",
        "\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2H4EEj-sD32"
      },
      "source": [
        "# Generate Predictions and Submit to Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2moYJhTWsOG-",
        "outputId": "57ab001f-be34-4d1f-cd72-eb27f8213b82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 82/82 [01:41<00:00,  1.23s/it]\n"
          ]
        }
      ],
      "source": [
        "# Make predictions\n",
        "\n",
        "# 1. Create a new object for CTCBeamDecoder with larger number of beams\n",
        "# 2. Get prediction string by decoding the results of the beam decoder\n",
        "\n",
        "torch.backends.cudnn.enabled = False\n",
        "TEST_BEAM_WIDTH = 10\n",
        "\n",
        "test_decoder    = CTCBeamDecoder(LABELS, beam_width=TEST_BEAM_WIDTH, log_probs_input= True)\n",
        "results = []\n",
        "\n",
        "model.eval()\n",
        "print(\"Testing\")\n",
        "for data in tqdm(test_loader):\n",
        "\n",
        "    x, lx   = data\n",
        "    x       = x.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        h, lh = model(x, lx)\n",
        "\n",
        "    prediction_string = decode_prediction(output = h, output_lens=lx, decoder = test_decoder)\n",
        "    #save the output in results array.\n",
        "    results.append(prediction_string)\n",
        "\n",
        "    del x, lx, h, lh\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "5q8CLHFiJ0uO"
      },
      "outputs": [],
      "source": [
        "results_unbatched = []\n",
        "for i in range(len(results)):\n",
        "  for j in range(len(results[i])):\n",
        "    results_unbatched.append(results[i][j])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "d70dvu_lsMlv"
      },
      "outputs": [],
      "source": [
        "data_dir = \"/content/random_submission.csv\"\n",
        "df = pd.read_csv(data_dir)\n",
        "df.label = results_unbatched\n",
        "df.to_csv('submission_7_test10.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1sZmEIs4yIz"
      },
      "outputs": [],
      "source": [
        "!kaggle competitions submit -c hw3p2-785-f24 -f early_submission_2.csv -m \"I made it!\"\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30786,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "vscode": {
      "interpreter": {
        "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "080ed6e31b5e458e8640db753ced634f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c7417433cfa427c934e09f47d71b05f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_080ed6e31b5e458e8640db753ced634f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c52be6375d3d4b32b32af82aab3f0a3b",
            "value": 0.836054148737092
          }
        },
        "8864d67a1e7f4921a1e312d0a3bcc53c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd3475f964b94415a0f453b39f86009a",
            "placeholder": "​",
            "style": "IPY_MODEL_d70a201ca3cc44069c00261de7d9757d",
            "value": "212.623 MB of 254.317 MB uploaded\r"
          }
        },
        "c52be6375d3d4b32b32af82aab3f0a3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cd3475f964b94415a0f453b39f86009a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d70a201ca3cc44069c00261de7d9757d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3aa82186e194b60958d5c8450fbc4bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa69b7611f6b49a88b6900b69fc8ba6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8864d67a1e7f4921a1e312d0a3bcc53c",
              "IPY_MODEL_5c7417433cfa427c934e09f47d71b05f"
            ],
            "layout": "IPY_MODEL_f3aa82186e194b60958d5c8450fbc4bf"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
